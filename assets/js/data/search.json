[
  
  {
    "title": "Ai Memory",
    "url": "/posts/ai-memory/",
    "categories": "",
    "tags": "",
    "date": "2025-11-18 00:00:00 +0800",
    "content": "Introduction to Memory Agent Architecture   (From The Rise and Potential of Large Language Model Based Agents: A Survey):     Perception: Information input.   Brain-Decision Making: Autonomous decision-making and planning, executing more complex tasks.   Brain-Memory &amp; Knowledge: Memory capability, storing Agentâ€™s knowledge and skills.   Action: Interacting with the external world, enabling Agents to autonomously complete more complex tasks through actions and perception.   The Role of Memory for Agents         Enable Agents to have continuous learning capabilities                     Summarize experiences from past interactions and learn from mistakes to improve task performance.                       Enable Agents to maintain conversational coherence and action consistency:                     Possess longer-range context management capabilities, maintaining consistent context in long conversations to ensure coherence.                       Avoid establishing facts that contradict previous ones, maintaining action consistency.                       Enable Agents to provide personalized services and user experiences        Memory VS RAG    AI Memory is more like the model's \"personal experience\" or \"conversation history\", focusing on continuity of internal state and personalization.   RAG is like equipping the model with a \"real-time reference library\", focusing on obtaining external, authoritative, and up-to-date information to improve answer accuracy and reliability.   Simply put, you can think of AI Memory as the model \"remembering\" what it has discussed with you, while RAG is the model \"looking up information\" when answering your questions to ensure correctness. An intelligent AI assistant will likely use both capabilities simultaneously.  Memory Implementation Mem0   Components         LLM (OpenAI, Ollama, Azure OpenAI, Gemini, DeepSeek, and 18 other types)            Vector databases     (Qdrant, Chroma, Pgvector, Milvus, and 19 other types)           Graph databases     (Neo4j, Memgraph, Neptune Analytics, Kuzu)           Embedding models     (OpenAI, Azure OpenAI, Ollama, and 10 other types)      Architecture         API interface layer           Intelligent processing layer: LLM reasoning + fact extraction + conflict resolution            Fact extraction: Using custom prompts to extract key information from conversations       Conflict resolution: Intelligently determining ADD/UPDATE/DELETE/NONE operations       Memory optimization: Avoiding redundancy, keeping information up-to-date                Dual storage architecture: Vector database + Graph database           Factory layer: LLM/Embedder/VectorStore Factory      Process Main Process    Retrieval Process    Source Code Analysis # mem0's concurrent dual-path retrieval implementation with concurrent.futures.ThreadPoolExecutor() as executor:     # Path 1: Vector database semantic search     future_memories = executor.submit(self._search_vector_store, ...)      # Path 2: Graph database relationship search       future_graph_entities = executor.submit(self.graph.search, ...)      # Get results     vector_memories = future_memories.result()     # Retrieval results     graph_relations = future_graph_entities.result()  # Graph path entity relationship results   # Return separately return {     \"results\": vector_memories,    # Vector path results     \"relations\": graph_relations   # Graph path entity relationship results  }   def search(     self,     query: str,     vectors: list[float],     limit: Optional[int] = 5,     filters: Optional[dict] = None, ) -&gt; List[OutputData]:          ...          # Filter conditions     if filters:         for k, v in filters.items():             filter_conditions.append(\"payload-&gt;&gt;%s = %s\")             filter_params.extend([k, str(v)])     filter_clause = \"WHERE \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"      # Execute SQL: scalar + vector     with self._get_cursor() as cur:         cur.execute(             f\"\"\"             SELECT id, vector &lt;=&gt; %s::vector AS distance, payload             FROM {self.collection_name}             {filter_clause}             ORDER BY distance             LIMIT %s             \"\"\",             (vectors, *filter_params, limit),         )          results = cur.fetchall()     ...   def search(self, query, filters, limit=100):     ...      # Step 1: LLM extracts entities     entity_type_map = self._retrieve_nodes_from_data(query, filters)     # Step 2: Graph search for related entities and relationships     search_output = self._search_graph_db(node_list=list(entity_type_map.keys()), filters=filters)      ...          # Step 3: BM25 reranks search results     bm25 = BM25Okapi(search_outputs_sequence)      tokenized_query = query.split(\" \")     reranked_results = bm25.get_top_n(tokenized_query, search_outputs_sequence, n=5)      ...  # Implementation of _search_graph_db graph search def _search_graph_db(self, node_list, filters, limit=100):          # Filter conditions     ...     node_props_str = ...          for node in node_list:         n_embedding = self.embedding_model.embed(node)              # Node retrieval based on vector similarity         cypher_query = f\"\"\"         -------Match candidate nodes-------         ---self.node_label = \":`__Entity__`\" if self.config.graph_store.config.base_label else \"\"         MATCH (n {self.node_label} })         WHERE n.embedding IS NOT NULL         -------Calculate cosine similarity-------         WITH n, round(2 * vector.similarity.cosine(n.embedding, $n_embedding) - 1, 4) AS similarity         WHERE similarity &gt;= $threshold         -------Find outgoing and incoming edges-------         CALL              WITH n             MATCH (n)-[r]-&gt;(m {self.node_label} })             RETURN n.name AS source, elementId(n) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, m.name AS destination, elementId(m) AS destination_id             UNION             WITH n               MATCH (n)&lt;-[r]-(m {self.node_label} })             RETURN m.name AS source, elementId(m) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, n.name AS destination, elementId(n) AS destination_id                  WITH distinct source, source_id, relationship, relation_id, destination, destination_id, similarity         RETURN source, source_id, relationship, relation_id, destination, destination_id, similarity         ORDER BY similarity DESC         LIMIT $limit         \"\"\"              params = {             \"n_embedding\": n_embedding,             \"threshold\": self.threshold,             \"user_id\": filters[\"user_id\"],             \"limit\": limit,         }                  ...                  ans = self.graph.query(cypher_query, params=params)         result_relations.extend(ans)          return result_relations   ğŸŒŸPowerMemğŸŒŸ GitHub: https://github.com/oceanbase/PowerMem Performance Improvements Multi-path Retrieval/Hybrid Search Flow Diagram     Full-text + Vector + Scalar            Version 441 and above can directly use kernel hybrid search           Fusion coarse ranking (RRF) + Fine ranking (Rerank model)            Rerank can improve accuracy by ~6%             Source Code Analysis:  def _hybrid_search(self, query: str, vectors: List[List[float]], limit: int = 5, filters: Optional[Dict] = None,                    fusion_method: str = \"rrf\", k: int = 60):      # Expand single-path limit by 3x     candidate_limit = limit * 3 if self.reranker else limit      # Step 1: Query separately     with ThreadPoolExecutor(max_workers=2) as executor:         # Vector         vector_future = executor.submit(self._vector_search, query, vectors, candidate_limit, filters)         # Full-text         fts_future = executor.submit(self._fulltext_search, query, candidate_limit, filters)          vector_results = vector_future.result()         fts_results = fts_future.result()      # Step 2: Fuse results for coarse ranking, limit results to 3 * limit     coarse_ranked_results = self._combine_search_results(         vector_results, fts_results, candidate_limit, fusion_method, k     )           # Step 3: Fine ranking (using qwen3-rerank model), limit results to limit     if self.reranker and query and coarse_ranked_results:         final_results = self._apply_rerank(query, coarse_ranked_results, limit)         return final_results     else:         return coarse_ranked_results[:limit]  # rerank def _apply_rerank(self, query: str, candidates: List[OutputData], limit: int) -&gt; List[OutputData]:          ...      # Call reranker to get reranked indices and scores     reranked_indices = self.reranker.rerank(query, documents, top_n=limit)          ...          # Special handling: high-scoring items at head and tail, because LLMs have recency effect     if len(final_results) &gt; 1:         reordered = [None] * len(final_results)         left = 0         right = len(final_results) - 1                  for i, result in enumerate(final_results):             if i % 2 == 0:                 # Even indices go to the left side                 reordered[left] = result                 left += 1             else:                 # Odd indices go to the right side                 reordered[right] = result                 right -= 1                  final_results = reordered      return final_results   Recency Effect Source:          Paper Title: Lost in the Middle: How Language Models Use Long Contexts     Authors: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Catasta, Percy Liang     Published: 2023     Link: https://arxiv.org/abs/2307.03172         U-shaped Performance Curve: The core finding of the paper is that when models need to retrieve information from long input texts (long contexts), their performance shows a clear \"U-shaped\" curve.  - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;Information located at the&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; beginning (Primacy) and end (Recency)&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; of the input sequence can be most accurately recalled and used by the model.&lt;/font&gt; - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;Information located in the&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; middle&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt; of the input sequence shows significantly decreased accuracy in recall and usage, as if \"lost in the middle\".&lt;/font&gt;   Paper Conclusions     Why must System Prompt be at the front? Because it needs to leverage the Primacy Effect. System instructions, role definitions, and core rules are global and must be firmly remembered by the model as the foundation of the entire conversation. Placing them at the beginning ensures they receive the highest attention weights.   Why must User Input be at the end? Because it needs to leverage the Recency Effect. The user's specific question is the instruction that the model needs to respond to immediately. Placing it at the end ensures it becomes the focus of the model's attention, directly driving the generation process.   Data Layering Purpose:  Use different vector dimensions and different index types for different data, layering the underlying data, achieving the most reasonable and efficient utilization through sub-table storage + intelligent routing. For example, main table 1536 dimensions, working memory table 512 dimensions, archive table 256 dimensions  Core Implementation Logic:  Use metadata (JSON) in database table fields to route and distribute based on different filters  Prerequisites:  Sub-table data and main table data have no intersection. For example, sub-table is shared memory, main table is private memory. When querying with type=XX, only one table will be queried  Routing Flow Diagram    OB Graph Implementation Graph databases are generally designed to solve querying large amounts of data with complex relationships, such as blockchain and decentralized applications. However, in RAG and memory retrieval environments, multi-hop requirements are often limited, and the amount of relationship data between entities is predictable. Therefore, relational databases can achieve the same purpose.  Table Structure  graph_entitiesã€Entitiesã€‘ â”œâ”€â”€ id (Primary Key) â”œâ”€â”€ name (Entity Name) â”œâ”€â”€ entity_type (Entity Type) â”œâ”€â”€ embedding (Vector Embedding) â”œâ”€â”€ created_at (Created At) â””â”€â”€ updated_at (Updated At)   graph_relationshipsã€Relationshipsã€‘ â”œâ”€â”€ id (Primary Key) â”œâ”€â”€ source_entity_id (Source Entity ID) â”œâ”€â”€ destination_entity_id (Destination Entity ID) â”œâ”€â”€ relationship_type (Relationship Type) â”œâ”€â”€ user_id (User ID) â”œâ”€â”€ agent_id (Agent ID) â”œâ”€â”€ run_id (Run ID) â”œâ”€â”€ created_at (Created At) â””â”€â”€ updated_at (Updated At)   Multi-hop Search  -- Hop 1 SELECT     ... FROM     (     SELECT         ...     FROM {self.relationship_table}     WHERE         source_entity_id IN ('alice_uuid', 'bob_uuid')  -- Seed entities         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;  -- Hop 2 -- Structure is identical, only parameters differ SELECT     ... FROM     (     SELECT         ...     FROM {self.relationship_table}     WHERE         source_entity_id IN ('openai_uuid', 'sf_uuid', 'charlie_uuid', ...)  -- Hop 1's destination_id         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.entity_table} e1 ON r.source_entity_id = e1.id JOIN {self.entity_table} e2 ON r.destination_entity_id = e2.id;  -- Hop 3 -- Structure is identical, only parameters differ ....     Example  Initial: current_sources = ['alice_uuid', 'bob_uuid']  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Hop 1: SQL-1                       \tâ”‚ â”‚  IN ('alice_uuid', 'bob_uuid')      â”‚ â”‚  Returns: 30 relationships         \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ Application layer loop prevention filtering            â”œâ”€ Accumulated results: 30            â”œâ”€ Check: 30 &lt; 100, continue            â”‚            â””â”€ Extract destination_id as next hop starting point               current_sources = ['openai_uuid', 'sf_uuid', ...]  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Hop 2: SQL-2                       \tâ”‚ â”‚  IN ('openai_uuid', 'sf_uuid', ...) â”‚ â”‚  Returns: 45 relationships          \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ Application layer loop prevention filtering (filter out 5 duplicate edges)            â”œâ”€ Accumulated results: 30 + 40 = 70            â”œâ”€ Check: 70 &lt; 100, continue            â”‚            â””â”€ Extract destination_id as next hop starting point               current_sources = ['sam_uuid', 'ca_uuid', ...]  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  Hop 3: SQL-3                       \tâ”‚ â”‚  IN ('sam_uuid', 'ca_uuid', ...)    â”‚ â”‚  Returns: 40 relationships          \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ Application layer loop prevention filtering (filter out 5 duplicate edges)            â”œâ”€ Accumulated results: 70 + 35 = 105            â”œâ”€ Check: 105 &gt;= 100 âœ… Limit satisfied!            â”‚            â””â”€ Return all_results[:100]   Features Intelligent Memory Management Through time decay, importance assessment, and spaced repetition mechanisms, like human memory, implementing adaptive forgetting mechanisms to prevent information overload.    Algorithm Foundation:    Where:     R: Memory Retention Rate, range [0, 1]   t: Time elapsed, unit: hours   S: Memory Strength, determined by decay_rate   Memory Addition Process    Memory Access Process    Memory Metadata Structure  {     \"intelligence\": {         \"importance_score\": 0.85,           # Importance score         \"memory_type\": \"long_term\",         # Memory type         \"initial_retention\": 0.85,          # Initial retention rate         \"decay_rate\": 0.1,                  # Decay rate         \"current_retention\": 0.72,          # Current retention rate         \"next_review\": \"2024-01-02T10:00:00\",  # Next review time, review will \"reset\" the forgetting curve, restarting decay from the review time point         \"review_schedule\": [...],           # Review schedule         \"last_reviewed\": \"2024-01-01T10:00:00\",  # Last reviewed time         \"review_count\": 2,                  # Review count         \"access_count\": 5,                  # Access count         \"reinforcement_factor\": 0.3         # Reinforcement factor, adjusts retention rate     },     \"memory_management\": {         \"should_promote\": False,            # Whether should promote         \"should_forget\": False,             # Whether should forget         \"should_archive\": False,            # Whether should archive         \"is_active\": True                   # Whether active     },     \"created_at\": \"2024-01-01T10:00:00\",   # Created at     \"updated_at\": \"2024-01-01T15:00:00\"    # Updated at }   References    ã€ŠThe Rise and Potential of Large Language Model Based Agents: A Surveyã€‹   ã€ŠLost in the Middle: How Language Models Use Long Contextsã€‹"
  },
  
  {
    "title": "Aiè®°å¿†",
    "url": "/posts/AI%E8%AE%B0%E5%BF%86/",
    "categories": "",
    "tags": "",
    "date": "2025-11-18 00:00:00 +0800",
    "content": "Memoryä»‹ç» Agentæ¶æ„   ï¼ˆæ¥è‡ª The Rise and Potential of Large Language Model Based Agents: A Surveyï¼‰ï¼š     æ„ŸçŸ¥èƒ½åŠ›ï¼ˆPerceptionï¼‰ï¼šä¿¡æ¯è¾“å…¥ã€‚   å†³ç­–èƒ½åŠ›ï¼ˆBrain-Decision Makingï¼‰ï¼šè‡ªä¸»å†³ç­–å’Œè§„åˆ’ï¼Œæ‰§è¡Œæ›´å¤æ‚çš„ä»»åŠ¡ã€‚   è®°å¿†èƒ½åŠ›ï¼ˆBrain-Memory &amp; Knowledgeï¼‰ï¼šå…·å¤‡è®°å¿†èƒ½åŠ›ï¼Œå­˜å‚¨ Agent çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚   è¡ŒåŠ¨èƒ½åŠ›ï¼ˆActionï¼‰ï¼šä¸å¤–ç•Œäº¤äº’ï¼Œé€šè¿‡è¡ŒåŠ¨ä¸æ„ŸçŸ¥è®© Agent èƒ½è‡ªä¸»å®Œæˆæ›´å¤šå¤æ‚ä»»åŠ¡ã€‚   è®°å¿†å¯¹Agentçš„ä½œç”¨         è®© Agent å…·å¤‡æŒç»­å­¦ä¹ èƒ½åŠ›                     ä»è¿‡å»ç»å†ä¸­æ€»ç»“ç»éªŒä»¥åŠä»é”™è¯¯ä¸­å­¦ä¹ ï¼ŒåŠ å¼ºä»»åŠ¡è¡¨ç°ã€‚                       è®© Agent èƒ½å¤Ÿä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œè¡ŒåŠ¨çš„ä¸€è‡´æ€§ï¼š                     å…·å¤‡æ›´è¿œè·ç¦»çš„ä¸Šä¸‹æ–‡ç®¡ç†èƒ½åŠ›ï¼Œåœ¨é•¿å¯¹è¯ä¸­èƒ½å¤Ÿä¿æŒä¸€è‡´çš„ä¸Šä¸‹æ–‡ä»è€Œä¿æŒè¿è´¯æ€§ã€‚                       é¿å…å»ºç«‹ä¸ä¹‹å‰ç›¸çŸ›ç›¾çš„äº‹å®ï¼Œä¿æŒè¡ŒåŠ¨çš„ä¸€è‡´æ€§ã€‚                       è®© Agent èƒ½å¤Ÿæä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡å’Œç”¨æˆ·ä½“éªŒ        Memory VS RAG    AI è®°å¿†æ›´åƒæ˜¯æ¨¡å‹çš„â€œä¸ªäººç»éªŒâ€æˆ–â€œå¯¹è¯å†å²â€ï¼Œä¾§é‡äºå†…éƒ¨çŠ¶æ€çš„æŒç»­æ€§å’Œä¸ªæ€§åŒ–ã€‚   RAG åˆ™åƒæ˜¯ç»™æ¨¡å‹é…å¤‡äº†ä¸€ä¸ªâ€œå®æ—¶å‚è€ƒå›¾ä¹¦é¦†â€ï¼Œä¾§é‡äºè·å–å¤–éƒ¨ã€æƒå¨ã€æœ€æ–°çš„ä¿¡æ¯ä»¥æé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚   ç®€å•æ¥è¯´ï¼Œä½ å¯ä»¥æŠŠ AI è®°å¿†ç†è§£ä¸ºæ¨¡å‹â€œè®°å¾—â€å’Œä½ èŠè¿‡ä»€ä¹ˆï¼Œè€Œ RAG æ˜¯æ¨¡å‹åœ¨å›ç­”ä½ é—®é¢˜æ—¶ï¼Œä¼šâ€œæŸ¥èµ„æ–™â€æ¥ç¡®ä¿ç­”æ¡ˆæ­£ç¡®ã€‚ä¸€ä¸ªæ™ºèƒ½çš„ AI åŠ©æ‰‹å¾ˆå¯èƒ½ä¼šåŒæ—¶è¿ç”¨è¿™ä¸¤ç§èƒ½åŠ›ã€‚  åŒºåˆ« | ç‰¹å¾ | AI è®°å¿† (AI Memory) | RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) | | :â€” | :â€” | :â€” | | æ ¸å¿ƒæœºåˆ¶ | é€šè¿‡å­˜å‚¨å’Œæ£€ç´¢æ¨¡å‹è‡ªèº«çš„äº¤äº’å†å²æˆ–å†…éƒ¨çŸ¥è¯†æ¥å½±å“æœªæ¥çš„è¾“å‡ºã€‚è®°å¿†å¯ä»¥æ˜¯çŸ­æœŸçš„ï¼ˆå¦‚å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰æˆ–é•¿æœŸçš„ï¼ˆå¦‚å‘é‡æ•°æ®åº“ä¸­å­˜å‚¨çš„è¿‡å¾€ç»éªŒï¼‰ã€‚ | åœ¨ç”Ÿæˆå“åº”å‰ï¼Œä»å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆå¦‚æ–‡æ¡£ã€æ•°æ®åº“ã€ç½‘é¡µï¼‰ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¹¶å°†æ£€ç´¢åˆ°çš„å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡è¾“å…¥ç»™æ¨¡å‹ã€‚ | | çŸ¥è¯†æ¥æº | ä¸»è¦æ¥æºäºæ¨¡å‹ä¸ç”¨æˆ·çš„äº¤äº’å†å²æˆ–ç³»ç»Ÿé¢„è®¾çš„é•¿æœŸè®°å¿†ã€‚ | æ¥æºäºå¤–éƒ¨ã€ç‹¬ç«‹çš„ã€é€šå¸¸æ˜¯ç»“æ„åŒ–çš„çŸ¥è¯†åº“ã€‚ | | ä¸»è¦ç›®çš„ | 1. ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼ˆå¦‚å¤šè½®å¯¹è¯ï¼‰ã€‚2. ä¸ªæ€§åŒ–ï¼ˆè®°ä½ç”¨æˆ·åå¥½ï¼‰ã€‚3. å­¦ä¹ å’Œé€‚åº”ï¼ˆä»è¿‡å¾€ç»éªŒä¸­å­¦ä¹ ï¼‰ã€‚ | 1. æä¾›æœ€æ–°ã€å‡†ç¡®çš„äº‹å®ä¿¡æ¯.2.  å‡å°‘å¹»è§‰ï¼ˆåŸºäºæ£€ç´¢åˆ°çš„çœŸå®ä¿¡æ¯ç”Ÿæˆï¼‰.3. æ‰©å±•æ¨¡å‹çŸ¥è¯†è¾¹ç•Œï¼ˆè®¿é—®è®­ç»ƒæ•°æ®ä¹‹å¤–çš„ä¿¡æ¯ï¼‰ã€‚ | | çŸ¥è¯†æ›´æ–° | è®°å¿†å¯ä»¥åŠ¨æ€æ›´æ–°ï¼Œä½†éœ€è¦ç®¡ç†è®°å¿†çš„å­˜å‚¨ã€æ£€ç´¢å’Œé—å¿˜ã€‚æ›´æ–°å¯èƒ½å½±å“æ¨¡å‹è¡Œä¸ºã€‚ | çŸ¥è¯†åº“å¯ä»¥ç‹¬ç«‹æ›´æ–°ï¼Œæ¨¡å‹æœ¬èº«æ— éœ€é‡æ–°è®­ç»ƒã€‚æ›´æ–°è§£è€¦äºæ¨¡å‹ã€‚ | | å®ç°æ–¹å¼ | + çŸ­æœŸè®°å¿†ï¼šç›´æ¥åˆ©ç”¨æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Windowï¼‰ã€‚+ é•¿æœŸè®°å¿†ï¼šä½¿ç”¨å‘é‡æ•°æ®åº“å­˜å‚¨å’Œæ£€ç´¢å…³é”®ä¿¡æ¯ç‰‡æ®µã€‚ | + æ£€ç´¢å™¨ï¼ˆå¦‚åŸºäºå‘é‡çš„ç›¸ä¼¼åº¦æœç´¢ï¼‰ã€‚+ ç”Ÿæˆå™¨ï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ã€‚+ çŸ¥è¯†åº“ï¼ˆæ–‡æ¡£é›†åˆï¼‰ã€‚ | | å…¸å‹åº”ç”¨ | èŠå¤©æœºå™¨äººä¿æŒå¯¹è¯è¿è´¯æ€§ã€ä¸ªæ€§åŒ–åŠ©æ‰‹ã€éœ€è¦é•¿æœŸå­¦ä¹ çš„AIä»£ç†ã€‚ | é—®ç­”ç³»ç»Ÿã€å®¢æœçŸ¥è¯†åº“ã€äº‹å®æ ¸æŸ¥ã€æŠ¥å‘Šç”Ÿæˆï¼ˆéœ€è¦å¼•ç”¨å¤–éƒ¨èµ„æ–™ï¼‰ã€‚ |  Memoryå®ç° Mem0   ç»„ä»¶         L     LMï¼ˆOpenAIã€Ollamaã€Azure OpenAIã€Geminiã€DeepSeekç­‰18ç§ï¼‰           å‘é‡æ•°     æ®åº“ï¼ˆQdrantã€Chromaã€Pgvectorã€Milvusç­‰19ç§ï¼‰           å›¾æ•°æ®åº“ï¼ˆ     Neo4jã€Memgraphã€Neptune Analyticsã€Kuzuï¼‰           Embedding åµŒå…¥æ¨¡å‹ï¼ˆ     OpenAIã€Azure OpenAIã€Ollamaç­‰10ç§ï¼‰      æ¶æ„         API æ¥å£å±‚           æ™ºèƒ½å¤„ç†å±‚ï¼šLLMæ¨ç† + äº‹å®æå– + å†²çªè§£å†³            äº‹å®æå–ï¼šä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯ä»å¯¹è¯ä¸­æå–å…³é”®ä¿¡æ¯       å†²çªè§£å†³ï¼šæ™ºèƒ½åˆ¤æ–­ADD/UPDATE/DELETE/NONEæ“ä½œ       è®°å¿†ä¼˜åŒ–ï¼šé¿å…å†—ä½™ï¼Œä¿æŒä¿¡æ¯æœ€æ–°                åŒå­˜å‚¨æ¶æ„ï¼šå‘é‡æ•°æ®åº“+å›¾æ•°æ®åº“           å·¥å‚å±‚ï¼šLLM/Embedder/VectorStroe  Factory      æµç¨‹ ä¸»æµç¨‹    å¬å›æµç¨‹    æºç è§£æ # mem0çš„å¹¶å‘åŒè·¯å¬å›å®ç° with concurrent.futures.ThreadPoolExecutor() as executor:     # è·¯å¾„1ï¼šå‘é‡æ•°æ®åº“è¯­ä¹‰æœç´¢     future_memories = executor.submit(self._search_vector_store, ...)      # è·¯å¾„2ï¼šå›¾æ•°æ®åº“å…³ç³»æœç´¢       future_graph_entities = executor.submit(self.graph.search, ...)      # è·å–ç»“æœ     vector_memories = future_memories.result()     # å¬å›ç»“æœ     graph_relations = future_graph_entities.result()  # å›¾è·¯å¾„å®ä½“å…³ç³»ç»“æœ   # åˆ†åˆ«è¿”å› return {     \"results\": vector_memories,    # å‘é‡è·¯å¾„ç»“æœ     \"relations\": graph_relations   # å›¾è·¯å¾„å®ä½“å…³ç³»ç»“æœ  }   def search(     self,     query: str,     vectors: list[float],     limit: Optional[int] = 5,     filters: Optional[dict] = None, ) -&gt; List[OutputData]:          ...          # è¿‡æ»¤æ¡ä»¶     if filters:         for k, v in filters.items():             filter_conditions.append(\"payload-&gt;&gt;%s = %s\")             filter_params.extend([k, str(v)])     filter_clause = \"WHERE \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"      # æ‰§è¡ŒSQLï¼šæ ‡é‡+å‘é‡     with self._get_cursor() as cur:         cur.execute(             f\"\"\"             SELECT id, vector &lt;=&gt; %s::vector AS distance, payload             FROM {self.collection_name}             {filter_clause}             ORDER BY distance             LIMIT %s             \"\"\",             (vectors, *filter_params, limit),         )          results = cur.fetchall()     ...   def search(self, query, filters, limit=100):     ...      # æ­¥éª¤1ï¼šllmæå–å®ä½“     entity_type_map = self._retrieve_nodes_from_data(query, filters)     # æ­¥éª¤2ï¼šå›¾æœç›¸å…³å®ä½“åŠå…³ç³»     search_output = self._search_graph_db(node_list=list(entity_type_map.keys()), filters=filters)      ...          # æ­¥éª¤3ï¼šbm25å¯¹æœç´¢ç»“æœé‡æ’     bm25 = BM25Okapi(search_outputs_sequence)      tokenized_query = query.split(\" \")     reranked_results = bm25.get_top_n(tokenized_query, search_outputs_sequence, n=5)      ...  # _search_graph_dbå›¾æœçš„å®ç° def _search_graph_db(self, node_list, filters, limit=100):          # è¿‡æ»¤æ¡ä»¶     ...     node_props_str = ...          for node in node_list:         n_embedding = self.embedding_model.embed(node)              # åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„èŠ‚ç‚¹æ£€ç´¢         cypher_query = f\"\"\"         -------åŒ¹é…å€™é€‰èŠ‚ç‚¹-------         ---self.node_label = \":`__Entity__`\" if self.config.graph_store.config.base_label else \"\"         MATCH (n {self.node_label} })         WHERE n.embedding IS NOT NULL         -------è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦-------         WITH n, round(2 * vector.similarity.cosine(n.embedding, $n_embedding) - 1, 4) AS similarity // denormalize for backward compatibility         WHERE similarity &gt;= $threshold         -------æ‰¾åˆ°å‡ºè¾¹ã€å…¥è¾¹-------         CALL              WITH n             MATCH (n)-[r]-&gt;(m {self.node_label} })             RETURN n.name AS source, elementId(n) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, m.name AS destination, elementId(m) AS destination_id             UNION             WITH n               MATCH (n)&lt;-[r]-(m {self.node_label} })             RETURN m.name AS source, elementId(m) AS source_id, type(r) AS relationship, elementId(r) AS relation_id, n.name AS destination, elementId(n) AS destination_id                  WITH distinct source, source_id, relationship, relation_id, destination, destination_id, similarity         RETURN source, source_id, relationship, relation_id, destination, destination_id, similarity         ORDER BY similarity DESC         LIMIT $limit         \"\"\"              params = {             \"n_embedding\": n_embedding,             \"threshold\": self.threshold,             \"user_id\": filters[\"user_id\"],             \"limit\": limit,         }                  ...                  ans = self.graph.query(cypher_query, params=params)         result_relations.extend(ans)          return result_relations   ğŸŒŸPowerMemğŸŒŸ GitHub: https://github.com/oceanbase/PowerMem æ€§èƒ½æå‡ å¤šè·¯å¬å›/æ··æœ æµç¨‹å›¾     å…¨æ–‡+å‘é‡+æ ‡é‡            441åŠä»¥ä¸Šå¯ä»¥ç›´æ¥ä½¿ç”¨å†…æ ¸çš„æ··æœ           èåˆç²—æ’ï¼ˆRRFï¼‰+ç²¾æ’ï¼ˆRerankæ¨¡å‹ï¼‰            rerankå¯å°†å‡†ç¡®ç‡æå‡ï½6%             æºç è§£æï¼š  def _hybrid_search(self, query: str, vectors: List[List[float]], limit: int = 5, filters: Optional[Dict] = None,                    fusion_method: str = \"rrf\", k: int = 60):      # å•è·¯limitæ‰©æˆ3å€     candidate_limit = limit * 3 if self.reranker else limit      # æ­¥éª¤ 1: åˆ†åˆ«æŸ¥è¯¢     with ThreadPoolExecutor(max_workers=2) as executor:         # å‘é‡         vector_future = executor.submit(self._vector_search, query, vectors, candidate_limit, filters)         # å…¨æ–‡         fts_future = executor.submit(self._fulltext_search, query, candidate_limit, filters)          vector_results = vector_future.result()         fts_results = fts_future.result()      # æ­¥éª¤ 2: ç»“æœèåˆè¿›è¡Œç²—æ’,ç»“æœé™åˆ¶ä¸º3 * limit     coarse_ranked_results = self._combine_search_results(         vector_results, fts_results, candidate_limit, fusion_method, k     )           # æ­¥éª¤ 3ï¼šè¿›è¡Œç²¾æ’ï¼ˆä½¿ç”¨qwen3-rerankæ¨¡å‹ï¼‰,ç»“æœé™åˆ¶ä¸ºlimit     if self.reranker and query and coarse_ranked_results:         final_results = self._apply_rerank(query, coarse_ranked_results, limit)         return final_results     else:         return coarse_ranked_results[:limit]  # rerank def _apply_rerank(self, query: str, candidates: List[OutputData], limit: int) -&gt; List[OutputData]:          ...      # Call reranker to get reranked indices and scores     reranked_indices = self.reranker.rerank(query, documents, top_n=limit)          ...          # ç‰¹æ®Šå¤„ç†ï¼šåˆ†æ•°é«˜çš„åœ¨å¤´å’Œå°¾ï¼Œå› ä¸ºLLMå…·æœ‰æ•ˆåº”è¿‘å› æ•ˆåº”     if len(final_results) &gt; 1:         reordered = [None] * len(final_results)         left = 0         right = len(final_results) - 1                  for i, result in enumerate(final_results):             if i % 2 == 0:                 # Even indices go to the left side                 reordered[left] = result                 left += 1             else:                 # Odd indices go to the right side                 reordered[right] = result                 right -= 1                  final_results = reordered      return final_results   è¿‘å› æ•ˆåº”æ•ˆåº”æ¥æºï¼š          è®ºæ–‡æ ‡é¢˜: Lost in the Middle: How Language Models Use Long Contexts     ä½œè€…: Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Catasta, Percy Liang     å‘å¸ƒ: 2023å¹´     é“¾æ¥: https://arxiv.org/abs/2307.03172         Uå‹æ€§èƒ½æ›²çº¿ï¼ˆU-shaped Performance Curveï¼‰: è®ºæ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯ï¼Œå½“æ¨¡å‹éœ€è¦ä»é•¿ç¯‡è¾“å…¥æ–‡æœ¬ï¼ˆé•¿ä¸Šä¸‹æ–‡ï¼‰ä¸­æ£€ç´¢ä¿¡æ¯æ—¶ï¼Œå…¶æ€§èƒ½å‘ˆç°å‡ºæ˜æ˜¾çš„â€œUå‹â€æ›²çº¿ã€‚  - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;ä½äºè¾“å…¥åºåˆ—&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt;å¼€å¤´ï¼ˆPrimacyï¼‰å’Œç»“å°¾ï¼ˆRecencyï¼‰&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt;çš„ä¿¡æ¯ï¼Œèƒ½è¢«æ¨¡å‹æœ€å‡†ç¡®åœ°å›å¿†å’Œä½¿ç”¨ã€‚&lt;/font&gt; - &lt;font style=\"color:rgb(13, 18, 57);\"&gt;ä½äºè¾“å…¥åºåˆ—&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt;ä¸­é—´&lt;/font&gt;**&lt;font style=\"color:rgb(13, 18, 57);\"&gt;çš„ä¿¡æ¯ï¼Œè¢«å›å¿†å’Œä½¿ç”¨çš„å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ï¼Œä»¿ä½›â€œåœ¨ä¸­é—´è¿·å¤±äº†â€ã€‚&lt;/font&gt;   è®ºæ–‡ç»“è®º     System Prompt ä¸ºä»€ä¹ˆå¿…é¡»åœ¨å‰ï¼Ÿ å› ä¸ºå®ƒéœ€è¦åˆ©ç”¨ Primacy Effect (é¦–å› æ•ˆåº”)ã€‚ç³»ç»ŸæŒ‡ä»¤ã€è§’è‰²å®šä¹‰ã€æ ¸å¿ƒè§„åˆ™æ˜¯å…¨å±€æ€§çš„ï¼Œå¿…é¡»è¢«æ¨¡å‹ç‰¢ç‰¢è®°ä½ï¼Œä½œä¸ºæ•´ä¸ªå¯¹è¯çš„åŸºçŸ³ã€‚æ”¾åœ¨å¼€å¤´ï¼Œèƒ½ç¡®ä¿å®ƒä»¬è·å¾—æœ€é«˜çš„æ³¨æ„åŠ›æƒé‡ã€‚   ç”¨æˆ·å½“å‰è¾“å…¥ (User Input) ä¸ºä»€ä¹ˆå¿…é¡»åœ¨åï¼Ÿ å› ä¸ºå®ƒéœ€è¦åˆ©ç”¨ Recency Effect (è¿‘å› æ•ˆåº”)ã€‚ç”¨æˆ·çš„å…·ä½“é—®é¢˜æ˜¯æ¨¡å‹éœ€è¦ç«‹å³å“åº”çš„æŒ‡ä»¤ã€‚æ”¾åœ¨ç»“å°¾ï¼Œèƒ½ç¡®ä¿å®ƒæˆä¸ºæ¨¡å‹æ³¨æ„åŠ›çš„ç„¦ç‚¹ï¼Œä»è€Œç›´æ¥é©±åŠ¨ç”Ÿæˆè¿‡ç¨‹ã€‚   æ•°æ®åˆ†å±‚ ç›®çš„ï¼š  é’ˆå¯¹ä¸åŒæ•°æ®é‡‡ç”¨ä¸åŒçš„å‘é‡ç»´åº¦ï¼Œå’Œä¸åŒçš„ç´¢å¼•ç±»å‹ï¼Œå¯¹åº•å±‚æ•°æ®è¿›è¡Œåˆ†å±‚ï¼Œé€šè¿‡å­è¡¨å­˜å‚¨+æ™ºèƒ½è·¯ç”±æ¥è¾¾åˆ°æœ€åˆç†ã€é«˜æ•ˆçš„åˆ©ç”¨ã€‚ä¾‹å¦‚ä¸»è¡¨1536ç»´ï¼Œå·¥ä½œè®°å¿†è¡¨512ç»´ï¼Œå½’æ¡£è¡¨256ç»´  æ ¸å¿ƒå®ç°é€»è¾‘ï¼š  åˆ©ç”¨æ•°æ®åº“è¡¨å­—æ®µä¸­metadataï¼ˆjsonï¼‰ï¼Œæ ¹æ®ä¸åŒçš„filteræ¥è¿›è¡Œè·¯ç”±åˆ†å‘  å‰ç½®æ¡ä»¶ï¼š  å­è¡¨çš„æ•°æ®ä¸ä¸»è¡¨çš„æ•°æ®æ²¡æœ‰äº¤é›†ï¼Œä¾‹å¦‚å­è¡¨ä¸ºå…±æœ‰è®°å¿†ï¼Œä¸»è¡¨ä¸ºç§æœ‰è®°å¿†ï¼Œå•è¯æŸ¥è¯¢type=XXæ—¶åªä¼šå»æŸ¥è¯¢ä¸€å¼ è¡¨  è·¯ç”±æµç¨‹å›¾    OB Graphå®ç° å›¾æ•°æ®åº“ä¸€èˆ¬æ˜¯ä¸ºäº†è§£å†³æŸ¥è¯¢å…·æœ‰å¤æ‚å…³ç³»çš„å¤§é‡æ•°æ®ï¼Œä¾‹å¦‚åƒåŒºå—é“¾å’Œå»ä¸­å¿ƒåŒ–åº”ç”¨ï¼Œä½†åœ¨RAGã€è®°å¿†æ£€ç´¢çš„ç¯å¢ƒä¸‹ï¼Œå¤šè·³çš„éœ€æ±‚å¾€å¾€æ˜¯æœ‰é™çš„ï¼Œå¹¶ä¸”å®ä½“ä¸å®ä½“ä¹‹é—´çš„å…³ç³»æ•°æ®é‡æ˜¯å¯é¢„æµ‹ï¼Œå› æ­¤ç”¨å…³ç³»æ•°æ®åº“ä¹Ÿèƒ½è¾¾åˆ°ç›¸åŒç›®çš„  è¡¨ç»“æ„  graph_entitiesã€å®ä½“ã€‘ â”œâ”€â”€ id (ä¸»é”®) â”œâ”€â”€ name (å®ä½“åç§°) â”œâ”€â”€ entity_type (å®ä½“ç±»å‹) â”œâ”€â”€ embedding (å‘é‡åµŒå…¥) â”œâ”€â”€ created_at (åˆ›å»ºæ—¶é—´) â””â”€â”€ updated_at (æ›´æ–°æ—¶é—´)   graph_relationshipsã€å…³ç³»ã€‘ â”œâ”€â”€ id (ä¸»é”®) â”œâ”€â”€ source_entity_id (æºå®ä½“ ID) â”œâ”€â”€ destination_entity_id (ç›®æ ‡å®ä½“ ID) â”œâ”€â”€ relationship_type (å…³ç³»ç±»å‹) â”œâ”€â”€ user_id (ç”¨æˆ· ID) â”œâ”€â”€ agent_id (Agent ID) â”œâ”€â”€ run_id (Run ID) â”œâ”€â”€ created_at (åˆ›å»ºæ—¶é—´) â””â”€â”€ updated_at (æ›´æ–°æ—¶é—´)   **å¤šè·³æœç´¢ **  -- ç¬¬1è·³ SELECT     ... FROM     (     SELECT         ...     FROM {self.å…³ç³»è¡¨}     WHERE         source_entity_id IN ('alice_uuid', 'bob_uuid')  -- ç§å­å®ä½“         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.å®ä½“è¡¨} e1 ON r.source_entity_id = e1.id JOIN {self.å®ä½“è¡¨} e2 ON r.destination_entity_id = e2.id;  -- ç¬¬2è·³ -- ç»“æ„å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å‚æ•°ä¸åŒ SELECT     ... FROM     (     SELECT         ...     FROM {self.å…³ç³»è¡¨}     WHERE         source_entity_id IN ('openai_uuid', 'sf_uuid', 'charlie_uuid', ...)  -- ç¬¬1è·³çš„destination_id         AND r.user_id = 'user123'     ORDER BY mentions DESC, created_at DESC     LIMIT 1000 ) AS r JOIN {self.å®ä½“è¡¨} e1 ON r.source_entity_id = e1.id JOIN {self.å®ä½“è¡¨} e2 ON r.destination_entity_id = e2.id;  -- ç¬¬3è·³ -- ç»“æ„å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å‚æ•°ä¸åŒ ....     ä¸¾ä¾‹è¯´æ˜  åˆå§‹: current_sources = ['alice_uuid', 'bob_uuid']  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ç¬¬1è·³: SQL-1                       \tâ”‚ â”‚  IN ('alice_uuid', 'bob_uuid')      â”‚ â”‚  è¿”å›: 30ä¸ªå…³ç³»                      \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ åº”ç”¨å±‚é˜²ç¯è¿‡æ»¤            â”œâ”€ ç´¯è®¡ç»“æœ: 30ä¸ª            â”œâ”€ æ£€æŸ¥: 30 &lt; 100, ç»§ç»­            â”‚            â””â”€ æå–destination_idä½œä¸ºä¸‹ä¸€è·³èµ·ç‚¹               current_sources = ['openai_uuid', 'sf_uuid', ...]  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ç¬¬2è·³: SQL-2                       \tâ”‚ â”‚  IN ('openai_uuid', 'sf_uuid', ...) â”‚ â”‚  è¿”å›: 45ä¸ªå…³ç³»                      \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ åº”ç”¨å±‚é˜²ç¯è¿‡æ»¤ (è¿‡æ»¤æ‰5ä¸ªé‡å¤è¾¹)            â”œâ”€ ç´¯è®¡ç»“æœ: 30 + 40 = 70ä¸ª            â”œâ”€ æ£€æŸ¥: 70 &lt; 100, ç»§ç»­            â”‚            â””â”€ æå–destination_idä½œä¸ºä¸‹ä¸€è·³èµ·ç‚¹               current_sources = ['sam_uuid', 'ca_uuid', ...]  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  ç¬¬3è·³: SQL-3                       \tâ”‚ â”‚  IN ('sam_uuid', 'ca_uuid', ...)    â”‚ â”‚  è¿”å›: 40ä¸ªå…³ç³»                      \tâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚            â”œâ”€ åº”ç”¨å±‚é˜²ç¯è¿‡æ»¤ (è¿‡æ»¤æ‰5ä¸ªé‡å¤è¾¹)            â”œâ”€ ç´¯è®¡ç»“æœ: 70 + 35 = 105ä¸ª            â”œâ”€ æ£€æŸ¥: 105 &gt;= 100 âœ… æ»¡è¶³limit!            â”‚            â””â”€ è¿”å› all_results[:100]   ç‰¹æ€§ æ™ºèƒ½è®°å¿†ç®¡ç† é€šè¿‡æ—¶é—´è¡°å‡ã€é‡è¦æ€§è¯„ä¼°å’Œé—´éš”å¤ä¹ æœºåˆ¶ï¼Œåƒäººç±»è®°å¿†ä¸€æ ·ï¼Œå®ç°è‡ªé€‚åº”é—å¿˜æœºåˆ¶ä»¥é˜²æ­¢ä¿¡æ¯è¿‡è½½ã€‚    ç®—æ³•åŸºç¡€ï¼š    å…¶ä¸­ï¼š     R: è®°å¿†ä¿ç•™ç‡ (Retention Rate)ï¼ŒèŒƒå›´ [0, 1]   t: æ—¶é—´é—´éš” (Time elapsed)ï¼Œå•ä½ï¼šå°æ—¶   S: è®°å¿†å¼ºåº¦ (Strength)ï¼Œç”±è¡°å‡ç‡(decay_rate)å†³å®š   è®°å¿†æ·»åŠ æµç¨‹    è®°å¿†è®¿é—®æµç¨‹    è®°å¿†å…ƒæ•°æ®ç»“æ„  {     \"intelligence\": {         \"importance_score\": 0.85,           # é‡è¦æ€§åˆ†æ•°         \"memory_type\": \"long_term\",         # è®°å¿†ç±»å‹         \"initial_retention\": 0.85,          # åˆå§‹ä¿ç•™ç‡         \"decay_rate\": 0.1,                  # è¡°å‡ç‡         \"current_retention\": 0.72,          # å½“å‰ä¿ç•™ç‡         \"next_review\": \"2024-01-02T10:00:00\",  # ä¸‹æ¬¡å¤ä¹ æ—¶é—´ï¼Œå¤ä¹ ä¼š\"é‡ç½®\"é—å¿˜æ›²çº¿ï¼Œä»å¤ä¹ æ—¶é—´ç‚¹é‡æ–°å¼€å§‹è¡°å‡         \"review_schedule\": [...],           # å¤ä¹ è®¡åˆ’         \"last_reviewed\": \"2024-01-01T10:00:00\",  # æœ€åå¤ä¹ æ—¶é—´         \"review_count\": 2,                  # å¤ä¹ æ¬¡æ•°         \"access_count\": 5,                  # è®¿é—®æ¬¡æ•°         \"reinforcement_factor\": 0.3         # å¼ºåŒ–å› å­ï¼Œå¯¹ä¿ç•™ç‡è¿›è¡Œè°ƒæ•´     },     \"memory_management\": {         \"should_promote\": False,            # æ˜¯å¦åº”è¯¥æå‡         \"should_forget\": False,             # æ˜¯å¦åº”è¯¥é—å¿˜         \"should_archive\": False,            # æ˜¯å¦åº”è¯¥å½’æ¡£         \"is_active\": True                   # æ˜¯å¦æ´»è·ƒ     },     \"created_at\": \"2024-01-01T10:00:00\",   # åˆ›å»ºæ—¶é—´     \"updated_at\": \"2024-01-01T15:00:00\"    # æ›´æ–°æ—¶é—´ }   å‚è€ƒ    ã€ŠThe Rise and Potential of Large Language Model Based Agents: A Surveyã€‹   ã€ŠLost in the Middle: How Language Models Use Long Contextsã€‹"
  }
  
]

